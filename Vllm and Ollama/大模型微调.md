# 大模型微调(Fine-tuning)

## 什么是大模型微调
大模型微调(Fine-tuning)是指在预训练大模型（如 GPT、LLaMA 等）的基础上，使用特定领域或任务的数据进行二次训练，使模型更好地适应特定应用场景的过程。

通过微调，可以在保留模型原有通用知识的基础上，增强其在特定领域的表现能力。

微调过程实质上是一种知识迁移，将预训练阶段获得的通用语言理解能力，转化为解决特定问题的能力。

与从零开始训练相比，微调利用了预训练模型中已有的知识结构，大大提高了训练效率并降低了资源需求。
## 为什么需要大模型微调

#### 弥补通用模型的不足

- 领域专业化：预训练模型虽具有广泛的通用知识，但在特定专业领域（如医疗、法律、金融等）的知识深度不足，微调可以增强模型在这些专业领域的表现。
- 任务针对性：通用模型在特定任务上的表现可能次优，如摘要生成、情感分析等，微调可以显著提升模型在特定任务上的性能。
- 
#### 解决资源与成本问题

- 降低计算成本：从头训练大模型需要巨大的计算资源，而微调只需要较少的计算资源即可完成。
- 减少数据需求：微调只需要相对较少的领域数据，而非亿级别的预训练数据集。

#### 改善模型行为

- 减少幻觉：微调可以减少模型产生虚假内容的倾向，提高输出的准确性和可靠性。
- 安全合规：通过微调可以使模型的输出更符合安全、道德和法律要求。
- 定制化输出：可以调整模型的输出风格、格式和内容，以满足特定应用需求。

## 微调分类及特点
#### 指令微调(Instruction Fine-Tuning，IFT)

**原理：** 使用高质量的任务指令数据，通过优化输入的指令（prompt）来引导模型的行为，使其适应不同的任务需求。

**特点：**

- 依赖大规模、高质量的任务指令数据。
- 适用于多任务学习，能够提升模型对不同任务的泛化能力。
- 不改变模型参数，仅通过优化指令进行调整。

**缺点：**

- 需要高质量的指令数据集。
- 在某些特定任务上效果可能不如全参数微调。

#### 全参数微调(Full Fine Tuning，FFT)
**原理：** 调整整个模型的所有参数，使其适应特定任务。

**特点：**

- 能够获得最优的任务适配性和性能。
- 适用于数据量充足、计算资源充裕的场景。
- 适用于长期部署的专用模型。

**缺点：**

- 计算资源消耗大，对存储和训练硬件要求高。
- 可能导致模型的灾难性遗忘（Catastrophic Forgetting）。

#### 参数高效微调（Parameter-Efficient Fine-Tuning，PEFT）
**原理：** 只调整部分参数（如低秩矩阵、适配器层等），降低计算开销，同时保持模型的原始能力。

**特点：**
- 计算成本较低，适用于资源受限环境。
- 适用于多任务场景，灵活性高。
- 通过少量可训练参数实现模型微调。

**缺点：**
- 可能损失一定的模型泛化能力。
- 需要针对不同任务选择合适的参数高效微调方法。

## 常见的微调技术
#### LoRA（Low-Rank Adaptation）
**原理：**

LoRA基于这样一个假设：模型适应过程中的参数更新矩阵往往是低秩的。因此，可以用两个小矩阵的乘积来近似权重更新，大幅减少可训练参数数量。

**核心：**
为原模型的权重矩阵W添加一个低秩分解矩阵ΔW=BA，其中B和A分别是低秩矩阵，秩r远小于原矩阵维度。

**实现关键步骤：**

1. 冻结预训练模型的原始权重
2. 为每个需要微调的权重矩阵W添加低秩矩阵对B和A
3. 仅训练这些低秩矩阵
4. 推理时将ΔW与原始权重W相加

#### QLoRA（Quantized Low-Rank Adaptation）
**原理：**
QLoRA结合了模型量化和LoRA技术，通过量化预训练权重来节省内存，同时使用LoRA进行参数高效微调。

**核心：**
将原始模型量化为4比特精度，同时使用LoRA进行微调，并采用分页优化等技术减少内存使用。

**实现关键步骤：**
1. 将预训练模型量化至4比特
2. 应用LoRA技术添加可训练的低秩参数
3. 使用NF4（Normal Float 4）量化方案
4. 采用双重量化技术进一步节省内存
5. 使用分页优化技术处理激活值

#### 适配器调整（Adapter Tuning）
**原理：**
在原始模型层之间插入小型可训练模块（适配器），原模型参数保持不变，只训练这些新增的适配器模块。

**核心：**
适配器通常由降维层、非线性激活函数和升维层组成，形成"瓶颈"结构，大幅减少参数量。

**实现关键步骤：**

1. 冻结预训练模型的所有参数
2. 在Transformer层后添加适配器模块
3. 适配器内部包含降维层和升维层
4. 仅训练适配器模块的参数
5. 添加残差连接以稳定训练

#### 前缀调整（Prefix Tuning）
**原理：**
在模型输入序列的前面添加一组可训练的连续向量（前缀），这些向量可以引导模型生成特定风格或领域的内容。

**核心：**
为模型的每一层添加可训练的前缀向量，这些向量在序列长度维度上扩展了注意力机制的上下文。

**实现关键步骤：**

1. 在每层自注意力机制的K和V矩阵前添加可训练的前缀向量
2. 冻结预训练模型的参数
3. 仅训练这些前缀向量
4. 使用参数化网络生成前缀，以提高稳定性

#### 提示调整（Prompt Tuning）
**原理：**
在输入嵌入层添加一组可学习的软提示向量，这些向量会与输入序列一起传入模型，引导模型行为。

**核心：**
只在输入层添加可训练的连续向量，而不是在每一层都添加，简化了前缀调整方法。

**实现关键步骤：**

1. 在输入嵌入序列前添加一组可训练的向量（软提示）
2. 冻结原始模型参数
3. 只训练这组软提示向量
4. 通过反向传播优化软提示向量

#### P-Tuning
**原理：**
在输入层添加少量可训练的伪标记（pseudo tokens），通过一个小型神经网络将这些标记转换为嵌入表示。

**核心：**
使用双向LSTM处理伪标记，生成上下文相关的连续提示表示。

**实现关键步骤：**

1. 添加少量可训练的伪标记
2. 使用双向LSTM处理这些伪标记
3. 将处理后的表示作为模型输入
4. 只训练伪标记和LSTM参数

#### P-Tuning v2
**原理：**
扩展了P-Tuning，将可训练的提示向量应用到模型的每一层，而不仅是输入层。

**核心：**
将深度可学习的连续提示应用于模型的所有层，像Prefix Tuning一样，但结构更简单高效。

**实现关键步骤：**

1. 在每层自注意力机制中添加可训练的提示向量
2. 冻结原始模型参数
3. 使用简单的结构，无需参数化网络
4. 优化所有层的提示向量

#### 插件式指令微调PILL（Pluggable Instruction Language Learning）
**原理：**
将指令理解能力模块化为可插拔组件，在保持原模型功能的同时增强模型对指令的理解和执行能力。

**核心：**
构建独立的指令理解模块，在不影响原始模型的情况下，增强模型对指令的处理能力。

**实现关键步骤：**

1. 设计专门的指令处理模块
2. 将该模块与原模型集成但保持结构独立
3. 使用指令数据集训练该模块
4. 实现模块的即插即用，便于在不同模型间迁移

#### SSF（Scaling & Shifting Your Features）微调
**原理：**
通过缩放和平移特征表示来调整模型行为，而不是直接修改原始权重。

**核心：**
为模型每层的特征表示添加可训练的缩放和平移参数，类似于批归一化的思想。

**实现关键步骤：**

1. 为每层特征添加缩放因子和平移因子
2. 冻结原始模型参数
3. 仅训练这些缩放和平移参数
4. 根据公式：y = αx + β调整特征表示

VLLM是一款经过优化的推理引擎，在令牌生成速度和内存管理效率上表现出色，是大规模AI应用的理想之选。

Ollama则是一个轻量级、易上手的框架，让在本地电脑上运行开源大语言模型变得更加简单。


#### 什么是VLLM？

VLLM（超大型语言模型）是SKYPILOT开发的推理优化框架，主要用于提升大语言模型在GPU上的运行效率。它的优势体现在以下几个方面：

快速令牌生成：采用连续批处理技术，让令牌生成速度大幅提升。
高效内存利用：借助PagedAttention技术，在处理大上下文窗口时，能有效控制GPU内存消耗。
无缝集成：与PyTorch、TensorFlow等主流深度学习平台兼容，可轻松融入AI工作流程。
VLLM深受AI研究人员和需要大规模高性能推理的企业青睐。


#### 什么是Ollama？

Ollama是一个本地大语言模型运行时环境，能简化开源AI模型的部署和使用流程。它具备以下特点：

预打包模型丰富：内置了LLaMA、Mistral、Falcon等多种模型。
硬件适配性强：针对日常使用的硬件进行了CPU和GPU推理优化，无论是MacBook、PC还是边缘设备，都能流畅运行AI模型。
操作便捷：提供简洁的API和命令行界面（CLI），开发人员只需简单配置，就能快速启动大语言模型。

对于想在个人电脑上尝试AI模型的开发人员和AI爱好者来说，Ollama是个不错的选择。

#### 关键性能指标分析


VLLM借助PagedAttention技术，在推理速度上优势明显，处理大上下文窗口时也能游刃有余。这让它成为聊天机器人、搜索引擎、AI写作辅助工具等高性能AI应用的首选。

Ollama的速度也还不错，但受限于本地硬件配置。在MacBook、PC和边缘设备上运行小型模型时表现良好，不过遇到超大模型就有些力不从心了。

结论：Ollama更适合初学者，而需要深度定制的开发人员则可以选择VLLM。


#### VLLM的最佳应用场景


企业AI应用：如客户服务聊天机器人、AI驱动的搜索引擎等。
云端高端GPU部署：适用于A100、H100、RTX 4090等高端GPU的云端大语言模型部署。
模型微调与定制：方便进行模型微调和运行自定义模型。
大上下文窗口需求：适用于对上下文窗口要求较高的应用。


不太适用的场景：个人笔记本电脑、日常AI实验。

#### Ollama的最佳应用场景

本地设备运行：无需借助云资源，就能在Mac、Windows或Linux系统的设备上运行大语言模型。
本地模型试验：不需要复杂的设置，就能在本地轻松试验各种模型。
简易API集成：开发人员可以通过简单的API将AI功能集成到应用程序中。
边缘计算应用：在边缘计算场景中表现出色。


不太适用的场景：大规模AI部署、高强度GPU计算任务。

总结：VLLM更适合AI工程师，而Ollama则是开发人员和AI爱好者的好帮手。

#### VLLM入门教程


安装依赖项：在命令行中输入pip install vllm，按提示完成安装。
在LLaMA模型上运行推理：在Python环境中，输入以下代码：

```python
from vllm import LLM
llm = LLM(model="meta-llama/Llama-2-7b")
output = llm.generate("What is VLLM?")
```

上述代码中，首先从vllm库中导入LLM类，然后创建LLM对象，并指定使用meta-llama/Llama-2-7b模型。
最后，使用generate方法输入问题“What is VLLM?”，就能得到模型的输出结果。

#### Ollama入门教程


安装Ollama（Mac/Linux系统）：在终端中输入brew install ollama，等待安装完成。
下载并运行模型：在终端输入ollama run mistral，即可下载并运行Mistral模型。
调用Ollama的API：在Python环境中，使用以下代码调用API：

```python
import requests
response = requests.post("http://localhost:11434/api/generate", json={"model": "mistral", "prompt": "Tell me a joke"})
print(response.json())
```


上述代码中，首先导入requests库，然后使用requests.post方法向本地的Ollama API发送请求，
请求地址为http://localhost:11434/api/generate，并在请求中指定使用的模型为mistral，
输入的提示内容为“Tell me a joke”。最后，打印API返回的结果。


[Ollama中文文档]: https://ollama.readthedocs.io/quickstart/

[vllm中文站]: https://vllm.hyper.ai/docs/getting-started/quickstart/